# Memory Management

## 1. Introduction and Definition
Memory management is a critical aspect of computer systems that involves the allocation, deallocation, and organization of memory resources. It ensures efficient use of memory while preventing memory leaks and fragmentation. Memory management is essential for both system performance and application stability.

Key terms:
- **Memory Allocation**: The process of assigning memory blocks to programs
- **Garbage Collection**: Automatic memory reclamation for unused objects
- **Memory Hierarchy**: Different levels of memory with varying speeds and capacities
- **Caching**: Storing frequently accessed data in faster memory
- **B-Tree**: A balanced tree data structure optimized for disk storage
- **Memory Leak**: Unreleased memory that can't be accessed or freed

## 2. Why and When to Use
Memory management is crucial when:
- Developing large-scale applications
- Working with limited memory resources
- Handling dynamic data structures
- Managing system resources
- Implementing database systems
- Optimizing application performance

## 3. Key Characteristics
- **Efficiency**: Optimal memory usage and allocation
- **Reliability**: Prevention of memory leaks and crashes
- **Scalability**: Ability to handle varying memory demands
- **Performance**: Fast allocation and deallocation
- **Fragmentation**: Management of memory fragmentation
- **Safety**: Protection against memory-related errors

## 4. Memory Allocation

### 4.1 Static vs Dynamic Allocation
- **Static Allocation**: Memory allocated at compile time
- **Dynamic Allocation**: Memory allocated at runtime

### 4.2 Memory Allocation Strategies
1. **First Fit**: Allocates the first block that's large enough
2. **Best Fit**: Allocates the smallest block that's large enough
3. **Worst Fit**: Allocates the largest block available
4. **Next Fit**: Similar to first fit but starts from the last allocation point

### 4.3 Implementation Example
```python
class MemoryBlock:
    def __init__(self, size, is_free=True):
        self.size = size
        self.is_free = is_free
        self.next = None

class MemoryManager:
    def __init__(self, total_size):
        self.head = MemoryBlock(total_size)
    
    def allocate(self, size):
        current = self.head
        while current:
            if current.is_free and current.size >= size:
                if current.size > size:
                    # Split block if it's larger than needed
                    new_block = MemoryBlock(current.size - size)
                    new_block.next = current.next
                    current.next = new_block
                    current.size = size
                current.is_free = False
                return current
            current = current.next
        return None  # No suitable block found
    
    def deallocate(self, block):
        block.is_free = True
        # Merge with adjacent free blocks
        current = self.head
        while current and current.next:
            if current.is_free and current.next.is_free:
                current.size += current.next.size
                current.next = current.next.next
            else:
                current = current.next
```

## 5. Garbage Collection

### 5.1 Types of Garbage Collection
1. **Reference Counting**
   - Tracks number of references to each object
   - Simple but can't handle circular references

2. **Mark and Sweep**
   - Marks all reachable objects
   - Sweeps unmarked objects
   - Can handle circular references

3. **Generational Collection**
   - Divides objects into generations
   - Collects younger generations more frequently
   - More efficient for long-running programs

### 5.2 Python's Garbage Collection
```python
import gc

# Enable garbage collection
gc.enable()

# Force garbage collection
gc.collect()

# Get garbage collection statistics
stats = gc.get_stats()
```

### 5.3 Memory Management in Python
- Reference counting for immediate cleanup
- Generational garbage collection for cyclic references
- Memory pools for small objects
- Automatic memory management

## 6. Memory Hierarchies and Caching

### 6.1 Memory Systems
1. **Registers**: Fastest, smallest, most expensive
2. **Cache Memory**: Fast, small, expensive
3. **Main Memory (RAM)**: Medium speed, medium size, medium cost
4. **Secondary Storage (Disk)**: Slow, large, inexpensive

### 6.2 Caching Strategies
1. **Direct Mapping**
   - Each block maps to one cache line
   - Simple but can cause conflicts

2. **Set Associative**
   - Each block maps to a set of cache lines
   - Better hit rate than direct mapping

3. **Fully Associative**
   - Each block can map to any cache line
   - Best hit rate but most complex

### 6.3 Cache Implementation Example
```python
class Cache:
    def __init__(self, size, associativity):
        self.size = size
        self.associativity = associativity
        self.sets = size // associativity
        self.cache = [[None] * associativity for _ in range(self.sets)]
        self.lru = [[0] * associativity for _ in range(self.sets)]
    
    def access(self, address):
        set_index = address % self.sets
        # Check if data is in cache
        for i in range(self.associativity):
            if self.cache[set_index][i] == address:
                # Update LRU
                self.lru[set_index][i] = max(self.lru[set_index]) + 1
                return True  # Cache hit
        
        # Cache miss - replace LRU entry
        lru_index = self.lru[set_index].index(min(self.lru[set_index]))
        self.cache[set_index][lru_index] = address
        self.lru[set_index][lru_index] = max(self.lru[set_index]) + 1
        return False  # Cache miss
```

## 7. External Searching and B-Trees

### 7.1 (a,b) Trees
- Balanced tree where each node has between a and b children
- All leaves are at the same level
- Used for external storage

### 7.2 B-Trees
- Special case of (a,b) trees where a = ⌈b/2⌉
- Optimized for disk storage
- Used in databases and file systems

### 7.3 B-Tree Implementation
```python
class BTreeNode:
    def __init__(self, leaf=True):
        self.leaf = leaf
        self.keys = []
        self.children = []

class BTree:
    def __init__(self, t):
        self.root = BTreeNode()
        self.t = t  # Minimum degree
    
    def insert(self, k):
        root = self.root
        if len(root.keys) == (2 * self.t) - 1:
            temp = BTreeNode(False)
            self.root = temp
            temp.children.insert(0, root)
            self.split_child(temp, 0)
            self.insert_nonfull(temp, k)
        else:
            self.insert_nonfull(root, k)
    
    def insert_nonfull(self, x, k):
        i = len(x.keys) - 1
        if x.leaf:
            while i >= 0 and k < x.keys[i]:
                i -= 1
            x.keys.insert(i + 1, k)
        else:
            while i >= 0 and k < x.keys[i]:
                i -= 1
            i += 1
            if len(x.children[i].keys) == (2 * self.t) - 1:
                self.split_child(x, i)
                if k > x.keys[i]:
                    i += 1
            self.insert_nonfull(x.children[i], k)
    
    def split_child(self, x, i):
        t = self.t
        y = x.children[i]
        z = BTreeNode(y.leaf)
        x.keys.insert(i, y.keys[t-1])
        
        z.keys = y.keys[t:]
        y.keys = y.keys[:t-1]
        
        if not y.leaf:
            z.children = y.children[t:]
            y.children = y.children[:t]
        
        x.children.insert(i + 1, z)
```

## 8. Memory Management Best Practices

### 8.1 General Guidelines
1. **Minimize Memory Allocations**
   - Reuse objects when possible
   - Use object pools for frequently allocated objects

2. **Prevent Memory Leaks**
   - Always free allocated memory
   - Use RAII (Resource Acquisition Is Initialization)
   - Implement proper cleanup in destructors

3. **Optimize Memory Usage**
   - Choose appropriate data structures
   - Use memory-efficient algorithms
   - Implement caching when beneficial

### 8.2 Language-Specific Considerations
1. **Python**
   - Use generators for large sequences
   - Implement `__del__` method carefully
   - Use context managers (`with` statement)

2. **C++**
   - Use smart pointers
   - Follow RAII principles
   - Avoid raw pointers when possible

3. **Java**
   - Use weak references for caches
   - Implement proper `finalize()` methods
   - Monitor garbage collection

## 9. Common Problems and Solutions

### Problem 1: Memory Leak Detection
```python
import tracemalloc

def detect_memory_leaks():
    tracemalloc.start()
    # Your code here
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    print("[ Top 10 memory leaks ]")
    for stat in top_stats[:10]:
        print(stat)
```

### Problem 2: Memory Fragmentation
```python
def defragment_memory(memory_blocks):
    # Sort blocks by address
    memory_blocks.sort(key=lambda x: x.address)
    
    # Merge adjacent free blocks
    i = 0
    while i < len(memory_blocks) - 1:
        if (memory_blocks[i].is_free and 
            memory_blocks[i+1].is_free and 
            memory_blocks[i].address + memory_blocks[i].size == 
            memory_blocks[i+1].address):
            memory_blocks[i].size += memory_blocks[i+1].size
            memory_blocks.pop(i+1)
        else:
            i += 1
```

## 10. Performance Considerations

### 10.1 Memory Access Patterns
1. **Spatial Locality**
   - Accessing nearby memory locations
   - Beneficial for cache performance

2. **Temporal Locality**
   - Reusing recently accessed data
   - Important for cache efficiency

### 10.2 Optimization Techniques
1. **Memory Pooling**
   - Pre-allocate memory blocks
   - Reduce allocation overhead

2. **Cache-Friendly Data Structures**
   - Align data to cache lines
   - Minimize pointer chasing

3. **Memory Mapping**
   - Map files directly to memory
   - Efficient for large files

## 11. Practice Problems
1. Implement a memory allocator using first-fit strategy
2. Create a simple garbage collector using mark-and-sweep
3. Implement a B-tree data structure
4. Design a cache with LRU replacement policy
5. Build a memory pool for frequently allocated objects
6. Implement memory defragmentation
7. Create a memory leak detector
8. Design a cache-friendly data structure
9. Implement memory mapping for file operations
10. Build a memory usage profiler 