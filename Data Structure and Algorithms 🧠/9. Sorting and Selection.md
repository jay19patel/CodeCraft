# Sorting and Selection

## 1. Introduction and Definition
Sorting is the process of arranging elements in a specific order (ascending or descending). Selection refers to finding the kth smallest or largest element in a collection. These operations are fundamental in computer science and are used in numerous applications.

Key terms:
- **Sorting**: Arranging elements in a specific order
- **Selection**: Finding the kth smallest or largest element
- **In-place sorting**: Sorting without using extra space
- **Stable sorting**: Preserving the relative order of equal elements
- **Comparison-based sorting**: Sorting by comparing elements
- **Non-comparison sorting**: Sorting without direct element comparisons
- **Divide-and-Conquer**: A problem-solving paradigm that breaks a problem into smaller subproblems

## 2. Why and When to Use
Sorting and selection algorithms are useful when:
- You need to organize data for efficient searching
- You need to find the median or other order statistics
- You need to identify duplicates or unique elements
- You need to prepare data for visualization
- You need to optimize data for specific operations

## 3. Key Characteristics
- **Time Complexity**: Varies from O(n log n) to O(n²) for comparison-based sorting
- **Space Complexity**: Ranges from O(1) for in-place algorithms to O(n) for algorithms requiring extra space
- **Stability**: Some algorithms preserve the relative order of equal elements, others don't
- **Adaptivity**: Some algorithms perform better on partially sorted data
- **Comparison vs. Non-comparison**: Different approaches to sorting based on element comparisons

## 4. Operations and Time Complexities

| Algorithm | Average Case | Worst Case | Best Case | Space Complexity | Stable | In-place |
|-----------|-------------|------------|-----------|------------------|--------|----------|
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes | No |
| Quick Sort | O(n log n) | O(n²) | O(n log n) | O(log n) | No | Yes |
| Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) | No | Yes |
| Insertion Sort | O(n²) | O(n²) | O(n) | O(1) | Yes | Yes |
| Selection Sort | O(n²) | O(n²) | O(n²) | O(1) | No | Yes |
| Bubble Sort | O(n²) | O(n²) | O(n) | O(1) | Yes | Yes |
| Counting Sort | O(n+k) | O(n+k) | O(n+k) | O(n+k) | Yes | No |
| Radix Sort | O(d(n+k)) | O(d(n+k)) | O(d(n+k)) | O(n+k) | Yes | No |
| Bucket Sort | O(n+n²/k) | O(n²) | O(n) | O(n+k) | Yes | No |
| Quick Select | O(n) | O(n²) | O(n) | O(1) | N/A | Yes |

## 5. Divide-and-Conquer

### Definition
Divide-and-Conquer is a problem-solving paradigm that:
1. Divides the problem into smaller subproblems
2. Conquers the subproblems by solving them recursively
3. Combines the solutions to the subproblems to solve the original problem

### Key Steps
1. **Divide**: Break the problem into smaller subproblems
2. **Conquer**: Solve the subproblems recursively
3. **Combine**: Merge the solutions of the subproblems

### Advantages
- Simplifies complex problems
- Enables parallel processing
- Often leads to efficient algorithms

### Disadvantages
- Recursion overhead
- May not be suitable for all problems
- Requires careful implementation for optimal performance

## 6. Merge-Sort

### Algorithm
Merge-Sort is a divide-and-conquer algorithm that:
1. Divides the array into two halves
2. Recursively sorts the two halves
3. Merges the sorted halves

### Pseudocode
```
MergeSort(A, left, right):
    if left < right:
        mid = (left + right) / 2
        MergeSort(A, left, mid)
        MergeSort(A, mid + 1, right)
        Merge(A, left, mid, right)

Merge(A, left, mid, right):
    n1 = mid - left + 1
    n2 = right - mid
    
    // Create temporary arrays
    L[1..n1+1] and R[1..n2+1]
    
    // Copy data to temporary arrays
    for i = 1 to n1:
        L[i] = A[left + i - 1]
    for j = 1 to n2:
        R[j] = A[mid + j]
    
    // Set sentinel values
    L[n1 + 1] = ∞
    R[n2 + 1] = ∞
    
    // Merge the temporary arrays back
    i = 1
    j = 1
    for k = left to right:
        if L[i] ≤ R[j]:
            A[k] = L[i]
            i = i + 1
        else:
            A[k] = R[j]
            j = j + 1
```

### Time Complexity Analysis
- **Divide**: O(1) to find the middle
- **Conquer**: 2T(n/2) for recursive calls
- **Combine**: O(n) for merging
- **Total**: T(n) = 2T(n/2) + O(n)
- **Solution**: O(n log n) using the Master Theorem

### Space Complexity
- O(n) for the temporary arrays used in merging

### Stability
- Merge-Sort is stable, preserving the relative order of equal elements

## 7. Implementation of Merge-Sort

### Python Implementation
```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    
    # Divide
    mid = len(arr) // 2
    left = arr[:mid]
    right = arr[mid:]
    
    # Conquer
    left = merge_sort(left)
    right = merge_sort(right)
    
    # Combine
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    
    # Compare elements from both arrays and merge them in sorted order
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    
    # Add remaining elements
    result.extend(left[i:])
    result.extend(right[j:])
    
    return result
```

### In-place Merge-Sort Implementation
```python
def merge_sort_inplace(arr, left=None, right=None):
    if left is None:
        left = 0
    if right is None:
        right = len(arr) - 1
    
    if left < right:
        mid = (left + right) // 2
        merge_sort_inplace(arr, left, mid)
        merge_sort_inplace(arr, mid + 1, right)
        merge_inplace(arr, left, mid, right)

def merge_inplace(arr, left, mid, right):
    # Create temporary arrays
    n1 = mid - left + 1
    n2 = right - mid
    L = [0] * n1
    R = [0] * n2
    
    # Copy data to temporary arrays
    for i in range(n1):
        L[i] = arr[left + i]
    for j in range(n2):
        R[j] = arr[mid + 1 + j]
    
    # Merge the temporary arrays back
    i = j = 0
    k = left
    
    while i < n1 and j < n2:
        if L[i] <= R[j]:
            arr[k] = L[i]
            i += 1
        else:
            arr[k] = R[j]
            j += 1
        k += 1
    
    # Copy remaining elements
    while i < n1:
        arr[k] = L[i]
        i += 1
        k += 1
    
    while j < n2:
        arr[k] = R[j]
        j += 1
        k += 1
```

### Optimizations
1. **Small subarrays**: Use insertion sort for small subarrays (typically < 10 elements)
2. **Skip merging**: If the last element of the left subarray is less than the first element of the right subarray, no merging is needed
3. **Block-based merging**: Use blocks to reduce memory access
4. **Parallel merging**: Utilize multiple processors for parallel merging

## 8. Quick-Sort

### Algorithm
Quick-Sort is a divide-and-conquer algorithm that:
1. Chooses a pivot element
2. Partitions the array around the pivot
3. Recursively sorts the subarrays

### Pseudocode
```
QuickSort(A, low, high):
    if low < high:
        pivot_index = Partition(A, low, high)
        QuickSort(A, low, pivot_index - 1)
        QuickSort(A, pivot_index + 1, high)

Partition(A, low, high):
    pivot = A[high]  // Choose the last element as pivot
    i = low - 1      // Index of smaller element
    
    for j = low to high - 1:
        // If current element is smaller than or equal to pivot
        if A[j] <= pivot:
            i = i + 1
            swap A[i] and A[j]
    
    swap A[i + 1] and A[high]
    return i + 1
```

### Time Complexity Analysis
- **Best Case**: O(n log n) when the pivot divides the array into equal halves
- **Average Case**: O(n log n) with a constant factor smaller than Merge-Sort
- **Worst Case**: O(n²) when the array is already sorted or reverse sorted

### Space Complexity
- O(log n) for the recursion stack in the average case
- O(n) in the worst case

### Stability
- Quick-Sort is not stable, as it may change the relative order of equal elements

### Pivot Selection Strategies
1. **First/Last element**: Simple but can lead to worst-case performance
2. **Median-of-three**: Choose the median of the first, middle, and last elements
3. **Random**: Choose a random element as pivot to avoid worst-case scenarios
4. **Median-of-medians**: A more complex strategy that guarantees O(n log n) performance

## 9. Implementation of Quick-Sort

### Python Implementation
```python
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    
    # Choose pivot (median-of-three)
    pivot = choose_pivot(arr)
    
    # Partition
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    
    # Recursively sort and combine
    return quick_sort(left) + middle + quick_sort(right)

def choose_pivot(arr):
    if len(arr) <= 2:
        return arr[0]
    
    # Median-of-three
    first = arr[0]
    last = arr[-1]
    mid = arr[len(arr) // 2]
    
    if first <= mid <= last or last <= mid <= first:
        return mid
    elif mid <= first <= last or last <= first <= mid:
        return first
    else:
        return last
```

### In-place Quick-Sort Implementation
```python
def quick_sort_inplace(arr, low=None, high=None):
    if low is None:
        low = 0
    if high is None:
        high = len(arr) - 1
    
    if low < high:
        # Partition and get pivot index
        pivot_index = partition(arr, low, high)
        
        # Recursively sort elements before and after partition
        quick_sort_inplace(arr, low, pivot_index - 1)
        quick_sort_inplace(arr, pivot_index + 1, high)

def partition(arr, low, high):
    # Choose pivot (median-of-three)
    mid = (low + high) // 2
    if arr[low] > arr[mid]:
        arr[low], arr[mid] = arr[mid], arr[low]
    if arr[low] > arr[high]:
        arr[low], arr[high] = arr[high], arr[low]
    if arr[mid] > arr[high]:
        arr[mid], arr[high] = arr[high], arr[mid]
    
    pivot = arr[mid]
    arr[mid], arr[high] = arr[high], arr[mid]
    
    # Partition
    i = low - 1
    
    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    
    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1
```

### Optimizations
1. **Small subarrays**: Use insertion sort for small subarrays
2. **Three-way partitioning**: Handle duplicate elements efficiently
3. **Tail recursion elimination**: Optimize the recursion stack
4. **Dual-pivot Quick-Sort**: Use two pivots for better performance

## 10. Comparing Sorting Algorithms

### Comparison Criteria
1. **Time Complexity**: How fast the algorithm runs
2. **Space Complexity**: How much extra memory the algorithm uses
3. **Stability**: Whether the algorithm preserves the relative order of equal elements
4. **In-place**: Whether the algorithm sorts without using extra space
5. **Adaptivity**: How well the algorithm performs on partially sorted data
6. **Cache Performance**: How well the algorithm utilizes CPU cache
7. **Parallelizability**: How easily the algorithm can be parallelized

### When to Use Each Algorithm

| Algorithm | Best For | Avoid When |
|-----------|----------|------------|
| Merge-Sort | Stable sorting, linked lists, external sorting | Small arrays, memory-constrained environments |
| Quick-Sort | General-purpose sorting, in-place sorting | Stability is required, arrays with many duplicates |
| Heap-Sort | In-place sorting, finding top-k elements | Stability is required, partially sorted data |
| Insertion-Sort | Small arrays, nearly sorted data | Large, unsorted arrays |
| Selection-Sort | Small arrays, minimizing swaps | Large arrays, stability is required |
| Bubble-Sort | Educational purposes, tiny arrays | Any practical application |
| Counting-Sort | Small range of integers | Large range of values, non-integer data |
| Radix-Sort | Strings, fixed-length integers | Variable-length data, floating-point numbers |
| Bucket-Sort | Uniformly distributed data | Skewed distributions |

### Performance Comparison
- **Small Arrays (< 50 elements)**: Insertion-Sort or Selection-Sort
- **Medium Arrays (50-1000 elements)**: Quick-Sort or Merge-Sort
- **Large Arrays (> 1000 elements)**: Quick-Sort with optimizations
- **Nearly Sorted Data**: Insertion-Sort or adaptive Quick-Sort
- **Stability Required**: Merge-Sort or stable Quick-Sort variants
- **Memory Constrained**: In-place Quick-Sort or Heap-Sort
- **External Sorting**: Merge-Sort
- **Parallel Processing**: Merge-Sort or parallel Quick-Sort

## 11. Selection

### Definition
Selection is the process of finding the kth smallest or largest element in an unsorted array. This is also known as the "order statistics" problem.

### Applications
- Finding the median
- Finding percentiles
- Finding the top-k elements
- Finding the minimum or maximum
- Finding the second smallest or second largest

### Naive Approach
1. Sort the array
2. Return the kth element
   - Time Complexity: O(n log n)
   - Space Complexity: O(1) if in-place sorting is used

### Better Approach: Quick-Select
Quick-Select is a selection algorithm based on Quick-Sort that finds the kth smallest element in an average time of O(n).

## 12. Quick-Select

### Algorithm
Quick-Select is a divide-and-conquer algorithm that:
1. Chooses a pivot element
2. Partitions the array around the pivot
3. Recursively selects from the appropriate subarray

### Pseudocode
```
QuickSelect(A, low, high, k):
    if low == high:
        return A[low]
    
    pivot_index = Partition(A, low, high)
    
    if k == pivot_index:
        return A[pivot_index]
    elif k < pivot_index:
        return QuickSelect(A, low, pivot_index - 1, k)
    else:
        return QuickSelect(A, pivot_index + 1, high, k)
```

### Time Complexity Analysis
- **Best Case**: O(n) when the pivot divides the array into equal halves
- **Average Case**: O(n) with a constant factor
- **Worst Case**: O(n²) when the array is already sorted or reverse sorted

### Space Complexity
- O(1) for in-place implementation
- O(log n) for the recursion stack in the average case
- O(n) in the worst case

### Implementation
```python
def quick_select(arr, k, low=None, high=None):
    if low is None:
        low = 0
    if high is None:
        high = len(arr) - 1
    
    if low == high:
        return arr[low]
    
    # Partition
    pivot_index = partition(arr, low, high)
    
    # If pivot is the kth element
    if k == pivot_index:
        return arr[pivot_index]
    # If k is less than pivot index, search in the left subarray
    elif k < pivot_index:
        return quick_select(arr, k, low, pivot_index - 1)
    # If k is greater than pivot index, search in the right subarray
    else:
        return quick_select(arr, k, pivot_index + 1, high)

def partition(arr, low, high):
    # Choose pivot (median-of-three)
    mid = (low + high) // 2
    if arr[low] > arr[mid]:
        arr[low], arr[mid] = arr[mid], arr[low]
    if arr[low] > arr[high]:
        arr[low], arr[high] = arr[high], arr[low]
    if arr[mid] > arr[high]:
        arr[mid], arr[high] = arr[high], arr[mid]
    
    pivot = arr[mid]
    arr[mid], arr[high] = arr[high], arr[mid]
    
    # Partition
    i = low - 1
    
    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    
    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1
```

### Optimizations
1. **Median-of-medians**: A more complex strategy that guarantees O(n) performance even in the worst case
2. **Small subarrays**: Use a simpler algorithm for small subarrays
3. **Random pivot**: Choose a random pivot to avoid worst-case scenarios

## 13. Real-world Applications
- Database indexing and query optimization
- Data compression
- Computer graphics (z-buffer algorithm)
- Operating system scheduling
- Network routing
- Data mining and machine learning
- Financial data analysis
- Bioinformatics (sequence alignment)

## 14. Common Problems and Solutions

### Problem 1: Find the Median of Two Sorted Arrays
```python
def find_median_sorted_arrays(nums1, nums2):
    # Ensure nums1 is the smaller array
    if len(nums1) > len(nums2):
        nums1, nums2 = nums2, nums1
    
    x, y = len(nums1), len(nums2)
    low, high = 0, x
    
    while low <= high:
        partitionX = (low + high) // 2
        partitionY = (x + y + 1) // 2 - partitionX
        
        maxLeftX = float('-inf') if partitionX == 0 else nums1[partitionX - 1]
        minRightX = float('inf') if partitionX == x else nums1[partitionX]
        maxLeftY = float('-inf') if partitionY == 0 else nums2[partitionY - 1]
        minRightY = float('inf') if partitionY == y else nums2[partitionY]
        
        if maxLeftX <= minRightY and maxLeftY <= minRightX:
            # We have partitioned the arrays correctly
            if (x + y) % 2 == 0:
                return (max(maxLeftX, maxLeftY) + min(minRightX, minRightY)) / 2
            else:
                return max(maxLeftX, maxLeftY)
        elif maxLeftX > minRightY:
            # We need to move partitionX to the left
            high = partitionX - 1
        else:
            # We need to move partitionX to the right
            low = partitionX + 1
    
    raise ValueError("Input arrays are not sorted")
```

### Problem 2: Sort an Almost Sorted Array
```python
def sort_almost_sorted(arr, k):
    # Use a min-heap to sort an array where each element is at most k positions away from its final position
    import heapq
    
    # Create a min-heap with the first k+1 elements
    heap = arr[:k+1]
    heapq.heapify(heap)
    
    # Process the remaining elements
    for i in range(k+1, len(arr)):
        # Extract the minimum element from the heap and place it in the result
        arr[i-(k+1)] = heapq.heappop(heap)
        # Add the next element to the heap
        heapq.heappush(heap, arr[i])
    
    # Process the remaining elements in the heap
    for i in range(len(arr)-k-1, len(arr)):
        arr[i] = heapq.heappop(heap)
    
    return arr
```

## 15. Advantages and Disadvantages

### Advantages of Sorting Algorithms
- **Merge-Sort**: Stable, guaranteed O(n log n) performance, good for external sorting
- **Quick-Sort**: In-place, cache-friendly, often faster in practice
- **Heap-Sort**: In-place, guaranteed O(n log n) performance, good for finding top-k elements
- **Insertion-Sort**: Simple, adaptive, good for small arrays
- **Selection-Sort**: Simple, in-place, good for minimizing swaps
- **Counting-Sort**: Linear time for small range of integers
- **Radix-Sort**: Linear time for fixed-length integers or strings
- **Bucket-Sort**: Linear time for uniformly distributed data

### Disadvantages of Sorting Algorithms
- **Merge-Sort**: Requires extra space, not in-place
- **Quick-Sort**: Not stable, worst-case O(n²) performance
- **Heap-Sort**: Not stable, poor cache performance
- **Insertion-Sort**: O(n²) time complexity
- **Selection-Sort**: O(n²) time complexity, not adaptive
- **Counting-Sort**: Requires known range of values, not suitable for large ranges
- **Radix-Sort**: Requires fixed-length data, not suitable for variable-length data
- **Bucket-Sort**: Performance depends on data distribution

### Advantages of Selection Algorithms
- **Quick-Select**: Average O(n) time complexity, in-place
- **Median-of-medians**: Guaranteed O(n) time complexity
- **Heap-based selection**: Good for finding top-k elements

### Disadvantages of Selection Algorithms
- **Quick-Select**: Worst-case O(n²) time complexity
- **Median-of-medians**: Complex implementation, high constant factor
- **Heap-based selection**: Requires extra space, not in-place

## 16. Practice Problems
1. Implement Merge-Sort and Quick-Sort
2. Find the kth largest element in an array
3. Sort an array of 0s, 1s, and 2s (Dutch National Flag problem)
4. Merge two sorted arrays in-place
5. Sort an almost sorted array (each element is at most k positions away from its final position)
6. Find the median of two sorted arrays
7. Implement a stable version of Quick-Sort
8. Sort a linked list using Merge-Sort
9. Implement Counting-Sort and Radix-Sort
10. Find the top-k elements in an array 