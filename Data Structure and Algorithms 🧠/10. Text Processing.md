# Text Processing

## 1. Introduction and Definition
Text processing involves the manipulation and analysis of text data, which is one of the most common forms of data in computer science. With the abundance of digitized text available today, efficient text processing algorithms are essential for applications ranging from search engines to natural language processing.

Key terms:
- **String**: A sequence of characters
- **Pattern Matching**: Finding occurrences of a pattern within a text
- **Text Compression**: Reducing the size of text data while preserving information
- **Trie**: A tree-like data structure for storing strings
- **Dynamic Programming**: A method for solving complex problems by breaking them down into simpler subproblems
- **Greedy Algorithm**: An algorithmic paradigm that makes the locally optimal choice at each step

## 2. Why and When to Use
Text processing algorithms are useful when:
- You need to search for patterns in large text documents
- You need to compress text data to save storage space
- You need to build efficient search indexes
- You need to align DNA or protein sequences
- You need to implement text editors or word processors
- You need to perform natural language processing tasks

## 3. Key Characteristics
- **Time Complexity**: Varies from O(n) to O(n²) depending on the algorithm
- **Space Complexity**: Ranges from O(1) to O(n) depending on the algorithm
- **Preprocessing**: Some algorithms require preprocessing the text or pattern
- **Online vs. Offline**: Some algorithms process text in a single pass, while others require multiple passes
- **Approximate vs. Exact Matching**: Some algorithms find exact matches, while others allow for errors

## 4. Operations and Time Complexities

| Algorithm | Average Case | Worst Case | Best Case | Space Complexity | Preprocessing Required |
|-----------|-------------|------------|-----------|------------------|------------------------|
| Brute Force | O(mn) | O(mn) | O(n) | O(1) | No |
| Boyer-Moore | O(n/m) | O(mn) | O(n/m) | O(k) | Yes |
| Knuth-Morris-Pratt | O(n) | O(n) | O(n) | O(m) | Yes |
| Rabin-Karp | O(n+m) | O(mn) | O(n+m) | O(1) | No |
| Suffix Tree | O(n) | O(n) | O(n) | O(n) | Yes |
| Huffman Coding | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes |
| Matrix Chain Multiplication | O(n³) | O(n³) | O(n³) | O(n²) | No |
| Sequence Alignment | O(mn) | O(mn) | O(mn) | O(mn) | No |
| Standard Trie | O(m) | O(m) | O(m) | O(ALPHABET_SIZE * m * n) | Yes |
| Compressed Trie | O(m) | O(m) | O(m) | O(n) | Yes |
| Suffix Trie | O(m) | O(m) | O(m) | O(n²) | Yes |

## 5. Abundance of Digitized Text

### The Digital Text Revolution
The amount of digitized text available today is unprecedented:
- **Web Content**: Billions of web pages containing trillions of words
- **Social Media**: Millions of posts, tweets, and messages generated daily
- **Digital Books**: Millions of books available in digital format
- **Scientific Literature**: Vast repositories of research papers and articles
- **Email and Documents**: Countless business and personal documents

### Challenges of Processing Large Text Corpora
1. **Storage**: Efficient storage and retrieval of massive text datasets
2. **Search**: Fast and accurate search across large text collections
3. **Analysis**: Extracting meaningful information from text data
4. **Compression**: Reducing storage requirements while maintaining accessibility
5. **Indexing**: Creating efficient data structures for quick access

### Applications
- **Search Engines**: Finding relevant information in the vast web
- **Natural Language Processing**: Understanding and generating human language
- **Information Retrieval**: Extracting relevant information from documents
- **Text Mining**: Discovering patterns and trends in text data
- **Bioinformatics**: Analyzing DNA and protein sequences

## 6. Notations for Strings and the Python str Class

### String Notations
- **Σ (Sigma)**: The alphabet, a finite set of symbols
- **Σ* (Sigma Star)**: The set of all strings over the alphabet Σ
- **ε (Epsilon)**: The empty string
- **|s|**: The length of string s
- **s[i]**: The character at position i in string s (0-indexed)
- **s[i:j]**: The substring of s from position i to j-1
- **s + t**: The concatenation of strings s and t
- **s^k**: The string s repeated k times

### Python str Class
The Python `str` class provides a rich set of methods for string manipulation:

```python
# String creation
s = "Hello, World!"
t = 'Python Programming'

# String length
length = len(s)  # 13

# String indexing and slicing
first_char = s[0]  # 'H'
last_char = s[-1]  # '!'
substring = s[0:5]  # 'Hello'
substring = s[:5]   # 'Hello'
substring = s[7:]   # 'World!'

# String concatenation
combined = s + " " + t  # 'Hello, World! Python Programming'

# String repetition
repeated = "Ha" * 3  # 'HaHaHa'

# String methods
uppercase = s.upper()  # 'HELLO, WORLD!'
lowercase = s.lower()  # 'hello, world!'
capitalized = s.capitalize()  # 'Hello, world!'
title_case = s.title()  # 'Hello, World!'
stripped = s.strip()  # 'Hello, World!'
replaced = s.replace('o', '0')  # 'Hell0, W0rld!'
split = s.split(', ')  # ['Hello', 'World!']
joined = '-'.join(['Hello', 'World'])  # 'Hello-World'

# String formatting
name = "Alice"
age = 30
formatted = f"My name is {name} and I am {age} years old."
formatted = "My name is {} and I am {} years old.".format(name, age)
formatted = "My name is %s and I am %d years old." % (name, age)

# String testing
is_alpha = s.isalpha()  # False (contains non-alphabetic characters)
is_digit = s.isdigit()  # False
is_space = s.isspace()  # False
starts_with = s.startswith("Hello")  # True
ends_with = s.endswith("!")  # True
contains = "World" in s  # True
```

### String Representation in Memory
- **ASCII**: 7-bit encoding for English characters (0-127)
- **Unicode**: Universal character encoding supporting multiple languages
- **UTF-8**: Variable-width encoding that is backward compatible with ASCII
- **UTF-16**: 16-bit encoding used by some systems
- **UTF-32**: Fixed-width 32-bit encoding

## 7. Pattern-Matching Algorithms

### 7.1 Brute Force

#### Algorithm
The brute force approach for pattern matching:
1. Align the pattern with the beginning of the text
2. Compare each character of the pattern with the corresponding character of the text
3. If all characters match, a match is found
4. If a mismatch is found, shift the pattern one position to the right
5. Repeat steps 2-4 until the pattern is found or the end of the text is reached

#### Pseudocode
```
BruteForce(T, P):
    n = length(T)
    m = length(P)
    
    for i = 0 to n - m:
        j = 0
        while j < m and T[i + j] == P[j]:
            j = j + 1
        
        if j == m:
            return i  // Match found at position i
    
    return -1  // No match found
```

#### Time Complexity
- **Best Case**: O(n) when the pattern is not found or found at the beginning
- **Average Case**: O(n) for random text and pattern
- **Worst Case**: O(mn) when the pattern is not found and many partial matches occur

#### Space Complexity
- O(1) as it doesn't require extra space

#### Implementation
```python
def brute_force(text, pattern):
    n, m = len(text), len(pattern)
    
    for i in range(n - m + 1):
        j = 0
        while j < m and text[i + j] == pattern[j]:
            j += 1
        
        if j == m:
            return i  # Match found at position i
    
    return -1  # No match found
```

### 7.2 The Boyer-Moore Algorithm

#### Algorithm
The Boyer-Moore algorithm is an efficient string searching algorithm that:
1. Aligns the pattern with the beginning of the text
2. Compares characters from right to left
3. Uses two heuristics to skip unnecessary comparisons:
   - **Bad Character Rule**: If a mismatch occurs, shift the pattern to align the rightmost occurrence of the mismatched character in the pattern with the mismatched character in the text
   - **Good Suffix Rule**: If a match is found for a suffix of the pattern, shift the pattern to align the rightmost occurrence of the matched suffix with the matched suffix in the text

#### Pseudocode
```
BoyerMoore(T, P):
    n = length(T)
    m = length(P)
    
    // Preprocess the pattern
    bad_char = compute_bad_char_table(P)
    good_suffix = compute_good_suffix_table(P)
    
    i = 0
    while i <= n - m:
        j = m - 1
        while j >= 0 and T[i + j] == P[j]:
            j = j - 1
        
        if j < 0:
            return i  // Match found at position i
        
        // Apply bad character rule
        bad_char_shift = j - bad_char[T[i + j]]
        
        // Apply good suffix rule
        good_suffix_shift = good_suffix[j]
        
        // Take the maximum of the two shifts
        i = i + max(bad_char_shift, good_suffix_shift)
    
    return -1  // No match found
```

#### Time Complexity
- **Best Case**: O(n/m) when the pattern is not found or found at the beginning
- **Average Case**: O(n/m) for random text and pattern
- **Worst Case**: O(mn) when the pattern is not found and many partial matches occur

#### Space Complexity
- O(k) where k is the size of the alphabet

#### Implementation
```python
def boyer_moore(text, pattern):
    n, m = len(text), len(pattern)
    
    # Preprocess the pattern
    bad_char = compute_bad_char_table(pattern)
    good_suffix = compute_good_suffix_table(pattern)
    
    i = 0
    while i <= n - m:
        j = m - 1
        while j >= 0 and text[i + j] == pattern[j]:
            j -= 1
        
        if j < 0:
            return i  # Match found at position i
        
        # Apply bad character rule
        bad_char_shift = j - bad_char.get(text[i + j], -1)
        
        # Apply good suffix rule
        good_suffix_shift = good_suffix[j]
        
        # Take the maximum of the two shifts
        i += max(bad_char_shift, good_suffix_shift)
    
    return -1  # No match found

def compute_bad_char_table(pattern):
    bad_char = {}
    for i in range(len(pattern) - 1):
        bad_char[pattern[i]] = i
    return bad_char

def compute_good_suffix_table(pattern):
    m = len(pattern)
    good_suffix = [0] * m
    
    # Simplified version - in practice, a more complex algorithm is used
    for i in range(m):
        good_suffix[i] = m
    
    return good_suffix
```

### 7.3 The Knuth-Morris-Pratt Algorithm

#### Algorithm
The Knuth-Morris-Pratt (KMP) algorithm is an efficient string searching algorithm that:
1. Preprocesses the pattern to create a partial match table (also known as the failure function or lps array)
2. Uses the partial match table to skip unnecessary comparisons
3. When a mismatch occurs, the algorithm knows how much of the pattern can be skipped

#### Pseudocode
```
KnuthMorrisPratt(T, P):
    n = length(T)
    m = length(P)
    
    // Preprocess the pattern
    lps = compute_lps_array(P)
    
    i = 0  // Index for text
    j = 0  // Index for pattern
    
    while i < n:
        if P[j] == T[i]:
            i = i + 1
            j = j + 1
        
        if j == m:
            return i - j  // Match found at position i - j
        
        elif i < n and P[j] != T[i]:
            if j != 0:
                j = lps[j - 1]
            else:
                i = i + 1
    
    return -1  // No match found

compute_lps_array(P):
    m = length(P)
    lps = [0] * m
    len = 0  // Length of the previous longest prefix suffix
    i = 1
    
    while i < m:
        if P[i] == P[len]:
            len = len + 1
            lps[i] = len
            i = i + 1
        else:
            if len != 0:
                len = lps[len - 1]
            else:
                lps[i] = 0
                i = i + 1
    
    return lps
```

#### Time Complexity
- **Preprocessing**: O(m) where m is the length of the pattern
- **Searching**: O(n) where n is the length of the text
- **Total**: O(m + n)

#### Space Complexity
- O(m) for the partial match table

#### Implementation
```python
def knuth_morris_pratt(text, pattern):
    n, m = len(text), len(pattern)
    
    # Preprocess the pattern
    lps = compute_lps_array(pattern)
    
    i = 0  # Index for text
    j = 0  # Index for pattern
    
    while i < n:
        if pattern[j] == text[i]:
            i += 1
            j += 1
        
        if j == m:
            return i - j  # Match found at position i - j
        
        elif i < n and pattern[j] != text[i]:
            if j != 0:
                j = lps[j - 1]
            else:
                i += 1
    
    return -1  # No match found

def compute_lps_array(pattern):
    m = len(pattern)
    lps = [0] * m
    length = 0  # Length of the previous longest prefix suffix
    i = 1
    
    while i < m:
        if pattern[i] == pattern[length]:
            length += 1
            lps[i] = length
            i += 1
        else:
            if length != 0:
                length = lps[length - 1]
            else:
                lps[i] = 0
                i += 1
    
    return lps
```

## 8. Dynamic Programming

### 8.1 Matrix Chain-Product

#### Problem Definition
Given a sequence of matrices, find the most efficient way to multiply these matrices together. The problem is not to actually perform the multiplications, but rather to decide the sequence of multiplications to minimize the number of operations.

#### Dynamic Programming Solution
1. Define the subproblems: Let `dp[i][j]` be the minimum number of operations needed to multiply the matrices from index i to j.
2. Define the recurrence relation: `dp[i][j] = min(dp[i][k] + dp[k+1][j] + dimensions[i-1] * dimensions[k] * dimensions[j])` for all k from i to j-1.
3. Define the base cases: `dp[i][i] = 0` for all i.
4. Fill the dp table in a bottom-up manner.

#### Pseudocode
```
MatrixChainMultiplication(dimensions):
    n = length(dimensions) - 1
    dp = create 2D array of size (n+1) x (n+1) initialized to 0
    
    // Fill the dp table in a bottom-up manner
    for length = 2 to n:
        for i = 1 to n - length + 1:
            j = i + length - 1
            dp[i][j] = infinity
            
            for k = i to j - 1:
                cost = dp[i][k] + dp[k+1][j] + dimensions[i-1] * dimensions[k] * dimensions[j]
                if cost < dp[i][j]:
                    dp[i][j] = cost
    
    return dp[1][n]
```

#### Time Complexity
- O(n³) where n is the number of matrices

#### Space Complexity
- O(n²) for the dp table

#### Implementation
```python
def matrix_chain_multiplication(dimensions):
    n = len(dimensions) - 1
    dp = [[0 for _ in range(n+1)] for _ in range(n+1)]
    
    # Fill the dp table in a bottom-up manner
    for length in range(2, n+1):
        for i in range(1, n - length + 2):
            j = i + length - 1
            dp[i][j] = float('inf')
            
            for k in range(i, j):
                cost = dp[i][k] + dp[k+1][j] + dimensions[i-1] * dimensions[k] * dimensions[j]
                if cost < dp[i][j]:
                    dp[i][j] = cost
    
    return dp[1][n]
```

### 8.2 DNA and Text Sequence Alignment

#### Problem Definition
Given two sequences (DNA, protein, or text), find the best alignment between them. The alignment can include matches, mismatches, insertions, and deletions.

#### Dynamic Programming Solution
1. Define the subproblems: Let `dp[i][j]` be the score of the best alignment between the first i characters of the first sequence and the first j characters of the second sequence.
2. Define the recurrence relation:
   - If the characters match: `dp[i][j] = dp[i-1][j-1] + match_score`
   - If the characters don't match: `dp[i][j] = max(dp[i-1][j-1] + mismatch_score, dp[i-1][j] + gap_score, dp[i][j-1] + gap_score)`
3. Define the base cases: `dp[0][0] = 0`, `dp[i][0] = i * gap_score`, `dp[0][j] = j * gap_score`.
4. Fill the dp table in a bottom-up manner.

#### Pseudocode
```
SequenceAlignment(seq1, seq2, match_score, mismatch_score, gap_score):
    m = length(seq1)
    n = length(seq2)
    dp = create 2D array of size (m+1) x (n+1) initialized to 0
    
    // Initialize the first row and column
    for i = 1 to m:
        dp[i][0] = i * gap_score
    for j = 1 to n:
        dp[0][j] = j * gap_score
    
    // Fill the dp table
    for i = 1 to m:
        for j = 1 to n:
            if seq1[i-1] == seq2[j-1]:
                dp[i][j] = dp[i-1][j-1] + match_score
            else:
                dp[i][j] = max(dp[i-1][j-1] + mismatch_score, dp[i-1][j] + gap_score, dp[i][j-1] + gap_score)
    
    return dp[m][n]
```

#### Time Complexity
- O(mn) where m and n are the lengths of the sequences

#### Space Complexity
- O(mn) for the dp table

#### Implementation
```python
def sequence_alignment(seq1, seq2, match_score=1, mismatch_score=-1, gap_score=-1):
    m, n = len(seq1), len(seq2)
    dp = [[0 for _ in range(n+1)] for _ in range(m+1)]
    
    # Initialize the first row and column
    for i in range(1, m+1):
        dp[i][0] = i * gap_score
    for j in range(1, n+1):
        dp[0][j] = j * gap_score
    
    # Fill the dp table
    for i in range(1, m+1):
        for j in range(1, n+1):
            if seq1[i-1] == seq2[j-1]:
                dp[i][j] = dp[i-1][j-1] + match_score
            else:
                dp[i][j] = max(dp[i-1][j-1] + mismatch_score, dp[i-1][j] + gap_score, dp[i][j-1] + gap_score)
    
    return dp[m][n]
```

## 9. Text Compression and the Greedy Method

### 9.1 The Huffman Coding Algorithm

#### Algorithm
Huffman coding is a lossless data compression algorithm that:
1. Builds a frequency table for each character in the text
2. Creates a binary tree where each leaf represents a character and the path from the root to the leaf represents the binary code for that character
3. The more frequent characters are assigned shorter codes
4. Encodes the text using the generated codes

#### Pseudocode
```
HuffmanCoding(text):
    // Build frequency table
    freq = create frequency table for each character in text
    
    // Create a min-heap of leaf nodes
    heap = create min-heap
    for each character c in freq:
        node = create leaf node with character c and frequency freq[c]
        insert node into heap
    
    // Build the Huffman tree
    while heap has more than one node:
        left = extract min from heap
        right = extract min from heap
        internal = create internal node with frequency left.freq + right.freq
        internal.left = left
        internal.right = right
        insert internal into heap
    
    // Generate Huffman codes
    root = extract min from heap
    codes = generate codes from root
    
    // Encode the text
    encoded = ""
    for each character c in text:
        encoded = encoded + codes[c]
    
    return encoded, root
```

#### Time Complexity
- O(n log n) where n is the number of unique characters

#### Space Complexity
- O(n) for the Huffman tree and frequency table

#### Implementation
```python
import heapq

class HuffmanNode:
    def __init__(self, char, freq):
        self.char = char
        self.freq = freq
        self.left = None
        self.right = None
    
    def __lt__(self, other):
        return self.freq < other.freq

def huffman_coding(text):
    # Build frequency table
    freq = {}
    for char in text:
        if char in freq:
            freq[char] += 1
        else:
            freq[char] = 1
    
    # Create a min-heap of leaf nodes
    heap = []
    for char, frequency in freq.items():
        heapq.heappush(heap, HuffmanNode(char, frequency))
    
    # Build the Huffman tree
    while len(heap) > 1:
        left = heapq.heappop(heap)
        right = heapq.heappop(heap)
        
        internal = HuffmanNode(None, left.freq + right.freq)
        internal.left = left
        internal.right = right
        
        heapq.heappush(heap, internal)
    
    # Generate Huffman codes
    root = heapq.heappop(heap)
    codes = {}
    
    def generate_codes(node, code=""):
        if node is None:
            return
        
        if node.char is not None:
            codes[node.char] = code
            return
        
        generate_codes(node.left, code + "0")
        generate_codes(node.right, code + "1")
    
    generate_codes(root)
    
    # Encode the text
    encoded = ""
    for char in text:
        encoded += codes[char]
    
    return encoded, root
```

### 9.2 The Greedy Method

#### Definition
The greedy method is an algorithmic paradigm that makes the locally optimal choice at each step, hoping that this will lead to a globally optimal solution.

#### Characteristics
1. **Greedy Choice Property**: A global optimal solution can be arrived at by making a locally optimal choice.
2. **Optimal Substructure**: An optimal solution to the problem contains optimal solutions to its subproblems.

#### Applications in Text Processing
1. **Huffman Coding**: Building an optimal prefix code for text compression
2. **Text Segmentation**: Breaking text into words or sentences
3. **Feature Selection**: Selecting the most important features for text classification
4. **Document Summarization**: Selecting the most important sentences for summarization

#### Advantages
- Simple to implement
- Often leads to efficient algorithms
- Works well for problems with the greedy choice property

#### Disadvantages
- Does not always lead to the globally optimal solution
- May require proof of correctness
- Not suitable for all problems

## 10. Tries

### 10.1 Standard Tries

#### Definition
A standard trie is a tree-like data structure for storing strings, where each node represents a character, and the path from the root to a node represents a prefix of some string.

#### Structure
- **Root Node**: The starting point of the trie
- **Internal Nodes**: Nodes that represent characters in the strings
- **Leaf Nodes**: Nodes that mark the end of a string
- **Edges**: Connections between nodes, labeled with characters

#### Operations
1. **Insertion**: Add a string to the trie
2. **Search**: Check if a string exists in the trie
3. **Prefix Search**: Find all strings with a given prefix
4. **Deletion**: Remove a string from the trie

#### Pseudocode
```
Insert(trie, string):
    current = trie.root
    for each character c in string:
        if c is not a child of current:
            current.children[c] = create new node
        current = current.children[c]
    current.is_end = true

Search(trie, string):
    current = trie.root
    for each character c in string:
        if c is not a child of current:
            return false
        current = current.children[c]
    return current.is_end

PrefixSearch(trie, prefix):
    current = trie.root
    for each character c in prefix:
        if c is not a child of current:
            return empty list
        current = current.children[c]
    return findAllStrings(current, prefix)
```

#### Time Complexity
- **Insertion**: O(m) where m is the length of the string
- **Search**: O(m) where m is the length of the string
- **Prefix Search**: O(p + k) where p is the length of the prefix and k is the number of strings with the prefix

#### Space Complexity
- O(ALPHABET_SIZE * m * n) where m is the average length of the strings and n is the number of strings

#### Implementation
```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()
    
    def insert(self, string):
        current = self.root
        for char in string:
            if char not in current.children:
                current.children[char] = TrieNode()
            current = current.children[char]
        current.is_end = True
    
    def search(self, string):
        current = self.root
        for char in string:
            if char not in current.children:
                return False
            current = current.children[char]
        return current.is_end
    
    def starts_with(self, prefix):
        current = self.root
        for char in prefix:
            if char not in current.children:
                return False
            current = current.children[char]
        return True
    
    def get_all_strings_with_prefix(self, prefix):
        current = self.root
        for char in prefix:
            if char not in current.children:
                return []
            current = current.children[char]
        
        result = []
        self._find_all_strings(current, prefix, result)
        return result
    
    def _find_all_strings(self, node, prefix, result):
        if node.is_end:
            result.append(prefix)
        
        for char, child in node.children.items():
            self._find_all_strings(child, prefix + char, result)
```

### 10.2 Compressed Tries

#### Definition
A compressed trie is a space-efficient variant of a standard trie where chains of nodes with only one child are compressed into a single node.

#### Structure
- **Standard Trie**: The base structure
- **Compression**: Chains of nodes with only one child are merged
- **Edge Labels**: Edges can be labeled with strings instead of single characters

#### Operations
1. **Insertion**: Add a string to the compressed trie
2. **Search**: Check if a string exists in the compressed trie
3. **Prefix Search**: Find all strings with a given prefix
4. **Deletion**: Remove a string from the compressed trie

#### Pseudocode
```
Insert(compressed_trie, string):
    current = compressed_trie.root
    i = 0
    while i < length(string):
        if no edge from current matches string[i:]:
            create new edge from current to new leaf node with label string[i:]
            return
        else:
            edge = find matching edge
            j = find longest common prefix of edge.label and string[i:]
            if j == length(edge.label):
                current = edge.target
                i = i + j
            else:
                split edge at position j
                create new edge from split point to new leaf node with label string[i+j:]
                return
    current.is_end = true
```

#### Time Complexity
- **Insertion**: O(m) where m is the length of the string
- **Search**: O(m) where m is the length of the string
- **Prefix Search**: O(p + k) where p is the length of the prefix and k is the number of strings with the prefix

#### Space Complexity
- O(n) where n is the total length of all strings

#### Implementation
```python
class CompressedTrieNode:
    def __init__(self):
        self.children = {}  # Maps string to node
        self.is_end = False

class CompressedTrie:
    def __init__(self):
        self.root = CompressedTrieNode()
    
    def insert(self, string):
        current = self.root
        i = 0
        while i < len(string):
            # Find the longest matching edge
            matched_edge = None
            matched_prefix_len = 0
            
            for edge_label, child in current.children.items():
                prefix_len = self._longest_common_prefix(string[i:], edge_label)
                if prefix_len > matched_prefix_len:
                    matched_prefix_len = prefix_len
                    matched_edge = (edge_label, child)
            
            if matched_edge is None:
                # No matching edge, create a new one
                current.children[string[i:]] = CompressedTrieNode()
                current.children[string[i:]].is_end = True
                return
            else:
                edge_label, child = matched_edge
                
                if matched_prefix_len == len(edge_label):
                    # Edge label is fully matched
                    current = child
                    i += matched_prefix_len
                else:
                    # Split the edge
                    prefix = edge_label[:matched_prefix_len]
                    suffix = edge_label[matched_prefix_len:]
                    
                    # Create a new node for the split
                    new_node = CompressedTrieNode()
                    new_node.children[suffix] = child
                    
                    # Update the current node
                    del current.children[edge_label]
                    current.children[prefix] = new_node
                    
                    # Create a new leaf node for the remaining part of the string
                    if i + matched_prefix_len < len(string):
                        new_node.children[string[i + matched_prefix_len:]] = CompressedTrieNode()
                        new_node.children[string[i + matched_prefix_len:]].is_end = True
                    else:
                        new_node.is_end = True
                    
                    return
        
        current.is_end = True
    
    def _longest_common_prefix(self, str1, str2):
        i = 0
        while i < len(str1) and i < len(str2) and str1[i] == str2[i]:
            i += 1
        return i
    
    def search(self, string):
        current = self.root
        i = 0
        while i < len(string):
            # Find the matching edge
            matched_edge = None
            matched_prefix_len = 0
            
            for edge_label, child in current.children.items():
                if string[i:].startswith(edge_label):
                    matched_edge = (edge_label, child)
                    matched_prefix_len = len(edge_label)
                    break
            
            if matched_edge is None:
                return False
            
            edge_label, child = matched_edge
            current = child
            i += matched_prefix_len
        
        return current.is_end
```

### 10.3 Suffix Tries

#### Definition
A suffix trie is a trie data structure that stores all suffixes of a given string. It is useful for string matching and pattern searching.

#### Structure
- **Root Node**: The starting point of the trie
- **Internal Nodes**: Nodes that represent characters in the suffixes
- **Leaf Nodes**: Nodes that mark the end of a suffix
- **Edges**: Connections between nodes, labeled with characters
- **Suffix Links**: Pointers from internal nodes to other nodes, used for efficient pattern matching

#### Operations
1. **Construction**: Build a suffix trie for a given string
2. **Pattern Matching**: Find all occurrences of a pattern in the string
3. **Longest Common Substring**: Find the longest substring that appears in multiple strings
4. **Longest Repeated Substring**: Find the longest substring that appears multiple times in the string

#### Pseudocode
```
ConstructSuffixTrie(string):
    trie = create new trie
    for i = 0 to length(string) - 1:
        suffix = string[i:]
        Insert(trie, suffix)
    return trie

FindPattern(suffix_trie, pattern):
    current = suffix_trie.root
    for each character c in pattern:
        if c is not a child of current:
            return empty list
        current = current.children[c]
    return findAllLeaves(current)
```

#### Time Complexity
- **Construction**: O(n²) where n is the length of the string
- **Pattern Matching**: O(m + k) where m is the length of the pattern and k is the number of occurrences

#### Space Complexity
- O(n²) where n is the length of the string

#### Implementation
```python
class SuffixTrieNode:
    def __init__(self):
        self.children = {}
        self.suffix_index = -1
        self.start_index = -1

class SuffixTrie:
    def __init__(self, text):
        self.root = SuffixTrieNode()
        self.text = text
        self._build_suffix_trie()
    
    def _build_suffix_trie(self):
        n = len(self.text)
        for i in range(n):
            self._insert_suffix(i)
    
    def _insert_suffix(self, start_index):
        current = self.root
        for i in range(start_index, len(self.text)):
            char = self.text[i]
            if char not in current.children:
                current.children[char] = SuffixTrieNode()
            current = current.children[char]
            current.suffix_index = start_index
            current.start_index = i - start_index + 1
    
    def search(self, pattern):
        current = self.root
        for char in pattern:
            if char not in current.children:
                return []
            current = current.children[char]
        
        return self._find_all_occurrences(current)
    
    def _find_all_occurrences(self, node):
        occurrences = []
        if node.suffix_index != -1:
            occurrences.append(node.suffix_index)
        
        for child in node.children.values():
            occurrences.extend(self._find_all_occurrences(child))
        
        return occurrences
```

### 10.4 Search Engine Indexing

#### Definition
Search engine indexing is the process of building data structures that map keywords to documents, enabling fast retrieval of relevant documents for a given query.

#### Components
1. **Crawler**: Collects web pages and extracts text content
2. **Tokenizer**: Breaks text into words or tokens
3. **Indexer**: Builds data structures for efficient retrieval
4. **Ranker**: Ranks documents based on relevance to the query

#### Data Structures
1. **Inverted Index**: Maps keywords to lists of documents containing those keywords
2. **Trie**: Stores keywords for prefix matching and autocomplete
3. **Suffix Array**: Enables substring search and pattern matching
4. **Bloom Filter**: Efficiently checks if a document contains a keyword

#### Operations
1. **Indexing**: Add documents to the index
2. **Searching**: Find documents matching a query
3. **Ranking**: Sort documents by relevance
4. **Updating**: Update the index when documents change

#### Pseudocode
```
IndexDocument(index, document):
    tokens = tokenize(document)
    for each token in tokens:
        if token not in index:
            index[token] = []
        if document not in index[token]:
            index[token].append(document)

Search(index, query):
    tokens = tokenize(query)
    result = intersection of index[tokens[0]] and index[tokens[1]] and ...
    return rank(result, query)
```

#### Time Complexity
- **Indexing**: O(n) where n is the number of tokens in the document
- **Searching**: O(k) where k is the number of documents containing the query terms
- **Ranking**: O(k log k) where k is the number of documents to rank

#### Space Complexity
- O(t) where t is the total number of tokens in all documents

#### Implementation
```python
class SearchEngine:
    def __init__(self):
        self.index = {}
    
    def index_document(self, doc_id, content):
        tokens = self._tokenize(content)
        for token in tokens:
            if token not in self.index:
                self.index[token] = set()
            self.index[token].add(doc_id)
    
    def search(self, query):
        tokens = self._tokenize(query)
        if not tokens:
            return []
        
        # Find documents containing all query terms
        result = self.index.get(tokens[0], set())
        for token in tokens[1:]:
            result = result.intersection(self.index.get(token, set()))
        
        return list(result)
    
    def _tokenize(self, text):
        # Simple tokenization: split by whitespace and convert to lowercase
        return [word.lower() for word in text.split()]
```

## 11. Real-world Applications
- **Search Engines**: Finding relevant information in large text collections
- **Text Editors**: Implementing features like search, replace, and syntax highlighting
- **Natural Language Processing**: Analyzing and understanding human language
- **Bioinformatics**: Analyzing DNA and protein sequences
- **Data Compression**: Reducing storage requirements for text data
- **Spam Filtering**: Identifying unwanted emails and messages
- **Plagiarism Detection**: Finding similar text in documents
- **Spell Checking**: Identifying and correcting misspelled words

## 12. Common Problems and Solutions

### Problem 1: Longest Common Substring
```python
def longest_common_substring(str1, str2):
    m, n = len(str1), len(str2)
    dp = [[0 for _ in range(n+1)] for _ in range(m+1)]
    max_length = 0
    end_index = 0
    
    for i in range(1, m+1):
        for j in range(1, n+1):
            if str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
                if dp[i][j] > max_length:
                    max_length = dp[i][j]
                    end_index = i
    
    return str1[end_index-max_length:end_index]
```

### Problem 2: Pattern Matching with Wildcards
```python
def pattern_matching_with_wildcards(text, pattern):
    m, n = len(text), len(pattern)
    dp = [[False for _ in range(n+1)] for _ in range(m+1)]
    
    # Empty pattern matches empty text
    dp[0][0] = True
    
    # Handle patterns starting with *
    for j in range(1, n+1):
        if pattern[j-1] == '*':
            dp[0][j] = dp[0][j-1]
    
    for i in range(1, m+1):
        for j in range(1, n+1):
            if pattern[j-1] == '*':
                dp[i][j] = dp[i-1][j] or dp[i][j-1]
            elif pattern[j-1] == '?' or text[i-1] == pattern[j-1]:
                dp[i][j] = dp[i-1][j-1]
    
    return dp[m][n]
```

## 13. Advantages and Disadvantages

### Advantages of Text Processing Algorithms
- **Efficiency**: Many algorithms have linear or near-linear time complexity
- **Scalability**: Can handle large text collections
- **Versatility**: Applicable to a wide range of problems
- **Robustness**: Can handle noisy or imperfect text data

### Disadvantages of Text Processing Algorithms
- **Memory Usage**: Some algorithms require significant memory
- **Preprocessing Overhead**: Some algorithms require expensive preprocessing
- **Approximation**: Some problems can only be solved approximately
- **Language Dependency**: Some algorithms work better for certain languages

## 14. Practice Problems
1. Implement the KMP algorithm for pattern matching
2. Build a Huffman coding tree for text compression
3. Create a trie data structure for efficient string storage and retrieval
4. Implement a suffix trie for substring search
5. Build a simple search engine index using an inverted index
6. Find the longest common substring between two strings
7. Implement pattern matching with wildcards
8. Build a spell checker using a trie
9. Implement a text compression algorithm
10. Create a simple plagiarism detector 